import requests
from lxml import html
from urllib.parse import urljoin
# å‘é€ GET è¯·æ±‚
url = "http://books.toscrape.com/"
response = requests.get(url)
current_url = url
# è¾“å‡ºç½‘é¡µæºç ï¼ˆå‰500å­—ç¬¦ç¤ºä¾‹ï¼‰
print("ç½‘é¡µæºç ï¼ˆéƒ¨åˆ†ï¼‰ï¼š")
print(response.text[:500])  # ä¸ºé¿å…æ§åˆ¶å°æ‹¥æŒ¤ï¼Œä»…æ‰“å°å‰500ä¸ªå­—ç¬¦
print("\n" + "="*80 + "\n")

# è¾“å‡ºå“åº”å¤´éƒ¨ä¿¡æ¯
print("å“åº”å¤´éƒ¨ä¿¡æ¯ï¼š")
for key, value in response.headers.items():
    print(f"{key}: {value}")
print("\n" + "="*80 + "\n")

# è¾“å‡ºè¯·æ±‚çŠ¶æ€ç 
print("è¯·æ±‚çŠ¶æ€ç ï¼š")
print(response.status_code)
print("\n" + "="*80 + "\n")

# è¾“å‡ºCookies
print("Cookiesï¼š")
for cookie in response.cookies:
    print(f"{cookie.name}: {cookie.value}")
print("\n" + "="*80 + "\n")

# ä½¿ç”¨lxmlè§£æç½‘é¡µ
tree = html.fromstring(response.text)

current_page = 1
max_pages = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•°ï¼ˆä¾‹å¦‚ 3ï¼‰ï¼š"))
while current_page <= max_pages:
    response = requests.get(current_url)
    tree = html.fromstring(response.text)

    print(f"\n==== æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µ ====")

    # æå–ä¹¦åå’Œä»·æ ¼
    book_titles = tree.xpath('//article[@class="product_pod"]/h3/a/@title')
    book_prices = tree.xpath('//article[@class="product_pod"]//p[@class="price_color"]/text()')

    for title, price in zip(book_titles, book_prices):
        print(f"ã€Š{title}ã€‹ - ä»·æ ¼ï¼š{price}")

    # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
    next_page = tree.xpath('//li[@class="next"]/a/@href')
    if next_page:
        next_href = next_page[0]
        current_url = urljoin(current_url, next_href)
        current_page += 1
    else:
        print("\nâœ… å·²ç»åˆ°è¾¾æœ€åä¸€é¡µï¼Œæå‰ç»“æŸçˆ¬å–ã€‚")
        break

print("\nğŸ‰ çˆ¬å–å®Œæˆã€‚")